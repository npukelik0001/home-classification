# -*- coding: utf-8 -*-
"""HouseSales.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xcn2wFn-xgS1-y9PzycqPshWQoBawVdu
"""

# Importing libraries and loading the data set
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, validation_curve, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')

import os
from dotenv import load_dotenv

# Load variables from .env file
load_dotenv()

# Get the values from the environment
kaggle_username = os.getenv('KAGGLE_USERNAME')
kaggle_key = os.getenv('KAGGLE_KEY')

# Set them for the Kaggle API to use
os.environ['KAGGLE_USERNAME'] = kaggle_username
os.environ['KAGGLE_KEY'] = kaggle_key

# Install Kaggle (if not done yet) (remove # if mot done and download it)
#!pip install -q kaggle

# Download the dataset
!kaggle datasets download -d harlfoxem/housesalesprediction --force

import zipfile

with zipfile.ZipFile("housesalesprediction.zip", 'r') as zip_ref:
    zip_ref.extractall(".")

"""Set random seed for reproducibility"""

np.random.seed(42)

# Load dataset into dataframe
df = pd.read_csv("kc_house_data.csv")

# Preview the dataset
print("DATASET OVERVIEW")
print(f"Dataset shape: {df.shape}")
print(f"Number of rows: {df.shape[0]}")
print(f"Number of columns: {df.shape[1]}")

# Looking at first 5 rows
print("FIRST 5 ROWS")
df.head()

# Looking at column names and data types
print("COLUMN NAMES AND DATA TYPES")
df.info()

# Summary statistics
df.describe()

# List of columns in the dataset
print("COLUMN NAMES")
list(df.columns)

"""Data Cleaning – I used a housing dataset from King County and began by removing any duplicated and missing rows to ensure clean input for modeling. Then I dropped columns that weren’t useful for prediction, such as id and date. I also created a new binary column called expensive_home to classify whether a home’s price is above the median (expensive) or not. The original price column was removed afterward to avoid data leakage."""

# Check dataframe for duplicated rows
duplicate_rows = df.duplicated().sum()
print(f"Number of duplicated rows: {duplicate_rows}")

# Check dataframe for null/missing values
print("MISSING VALUES ANALYSIS")
missing_data = df.isnull().sum()
missing_summary = pd.DataFrame({'Missing Data': missing_data})
missing_summary

"""View Grade Distribution & Create Target Column"""

# View grade distribution sorted in ascending order
grade_counts = df['grade'].value_counts().sort_index(ascending=True)
print("GRADE COUNTS (HIGH TO LOW GRADE):")
print(grade_counts)

# Filtering only homes built after 2000
print("\nBEFORE YEAR BUILT FILTERING")
print(f"Rows: {df.shape[0]:,}")
print(f"Columns: {df.shape[1]}")
print("Earliest year built:", df['yr_built'].min())
print("Latest year built:", df['yr_built'].max())

df_filtered = df[df['yr_built'] > 2000].copy()

print("\nAFTER YEAR BUILT FILTERING")
print(f"Rows: {df_filtered.shape[0]:,}")
print(f"Columns: {df_filtered.shape[1]}")
print("Remaining year range:", df_filtered['yr_built'].min(), "to", df_filtered['yr_built'].max())

# Create binary classification target column: 1 = expensive, 0 = not expensive
median_price = df_filtered['price'].median()
df_filtered['expensive_home'] = (df_filtered['price'] > median_price).astype(int)

# Removing unnecessary columns
print("COLUMN NAMES AFTER CLEANUP")

# Define all columns to drop
drop_columns = ['id', 'date', 'price']

# Drop them and make a copy
df_cleaned = df.drop(columns=drop_columns).copy()

# Confirm cleanup
print(list(df_cleaned.columns))

# Create a clean copy from the filtered data
df_cleaned = df_filtered.copy()

"""Exploratory Data Analysis, Data Visualization + Business Questions: Explore the target variable expensive_home and key housing features that may influence it."""

# Set up the plotting style
plt.style.use('default')
fig = plt.figure(figsize=(20, 15))

# Visualization #1: Target Distribution Pie Chart
plt.subplot(3, 4, 1)
target_counts = df_filtered['expensive_home'].value_counts()
labels = ['Not Expensive (0)', 'Expensive (1)']
plt.pie(target_counts.values, labels=labels, autopct='%1.1f%%', startangle=90, colors=['#66b3ff','#ff9999'])
plt.title('Distribution of Expensive vs Not Expensive Homes')

# Target variable distribution (expensive_home)
plt.figure(figsize=(12, 4))

# Bar chart of class distribution
plt.subplot(1, 2, 1)
sns.countplot(x='expensive_home', data=df_cleaned, palette='pastel')
plt.title('Distribution of Expensive vs Non-Expensive Homes')
plt.xlabel('Expensive Home (0 = No, 1 = Yes)')
plt.ylabel('Count')

# Box plot of a key feature by target (e.g., sqft_living)
plt.subplot(1, 2, 2)
sns.boxplot(data=df_cleaned, x='expensive_home', y='sqft_living', palette='pastel')
plt.title('Living Space by Expensive Classification')
plt.xlabel('Expensive Home (0 = No, 1 = Yes)')
plt.ylabel('sqft_living')

plt.tight_layout()
plt.show()

# Summary stats
print("Expensive Home Statistics (by Class):")
print(df_cleaned['expensive_home'].value_counts())
print("\nProportions:")
print(df_cleaned['expensive_home'].value_counts(normalize=True).round(2))

#Visual Interpretations
print("Visual Interpretations:")
print("1. Around 59% of the homes are classified as expensive, showing a slightly imbalanced but acceptable target distribution.")
print("2. Expensive homes tend to have significantly larger living space, with higher median and wider spread in sqft_living.")
print("3. The expensive category contains more extreme outliers in square footage, which may influence model performance.")

# Standalone heatmap with better size
plt.figure(figsize=(12, 10))
corr_matrix = df_cleaned.corr(numeric_only=True)
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', center=0)
plt.title("Feature Correlation Matrix")
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# Visual Interpretations
print("Visual Interpretations:")
print("1. The strongest correlation with expensive homes is sqft_living (r = 0.49), indicating that larger homes are more likely to be expensive.")
print("2. Grade and bathrooms also show strong positive correlations with expensive_home (r = 0.45 and r = 0.38), suggesting higher-rated homes and more bathrooms often increase price.")
print("3. Zipcode and long are negatively correlated with expensive_home (r = -0.27 and r = -0.26), which may reflect location-based pricing trends within King County.")

# Drop features with very low correlation to the target
# These features have weak or no relationship with expensive_home and may introduce noise
drop_low_corr = ['condition', 'id', 'floors', 'yr_renovated', 'sqft_lot']

# We are keeping 'waterfront' even though its correlation is low, since it's a high-impact feature in real estate
df_cleaned = df_cleaned.drop(columns=drop_low_corr)

print("Dropped low-correlation features:", drop_low_corr)
print("Remaining columns:", list(df_cleaned.columns))

# Get top correlated features with expensive_home (excluding the target itself)
expensive_corr = df_cleaned.corr(numeric_only=True)['expensive_home'].abs().drop('expensive_home').sort_values(ascending=False)

# Strip plots of top 4 features most correlated with expensive_home
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('Relationships Between Key Features and Expensive Home Classification')

# Get top 4 most correlated features
top_features = expensive_corr.head(4).index

for i, feature in enumerate(top_features):
    row = i // 2
    col = i % 2

    # Use stripplot for classification + numeric comparison
    sns.stripplot(ax=axes[row, col], x='expensive_home', y=feature, data=df_cleaned, alpha=0.5, jitter=True, palette='pastel')
    axes[row, col].set_xlabel("Expensive Home (0 = No, 1 = Yes)")
    axes[row, col].set_ylabel(feature)
    axes[row, col].set_title(f'{feature} vs Expensive Home (r={expensive_corr[feature]:.3f})')

plt.tight_layout()
plt.show()

print("Visual Interpretations:")
print("1. Grade has the strongest visual separation — expensive homes generally have higher grades, often clustered at 9 or above.")
print("2. Square footage (both sqft_living and sqft_above) increases significantly for expensive homes, showing clear vertical separation.")
print("3. Latitude also shows a pattern — expensive homes tend to be located at higher latitudes within the county, likely reflecting location-based value.")

"""Feature Engineering - Create new column: house_age"""

# This gives the age of the home as of 2025

df_cleaned['house_age'] = 2025 - df_cleaned['yr_built']

# Create sqft_ratio to see how the newer surrounding area compares to original home
df_cleaned['sqft_ratio'] = df_cleaned['sqft_living15'] / df_cleaned['sqft_living']

print("Feature Engineering:")
print("→ Created 'house_age' based on build year.")
print("→ Created 'sqft_ratio' to compare current living area to surrounding homes.")

# View the result
df_cleaned.head()

"""One-Hot Encoding"""

# Bin house_age into labeled categories
df_cleaned['house_age_group'] = pd.cut(
    df_cleaned['house_age'],
    bins=[0, 10, 30, 70, float('inf')],
    labels=['New', 'Modern', 'Old', 'Historic']
)

# One-hot encode house_age_group (drop first to avoid dummy trap)
df_encoded = pd.get_dummies(df_cleaned, columns=['house_age_group'], drop_first=True)

# Drop any one-hot bins that have 0 rows (safety)
unused_bins = ['house_age_group_Old', 'house_age_group_Historic']
df_encoded = df_encoded.drop(columns=unused_bins, errors='ignore')

print("One-hot encoding for house_age_group complete.")
print(f"New shape: {df_encoded.shape}")
print(f"New columns: {list(df_encoded.columns)}")

df_cleaned.head()

df_cleaned.to_csv("df_cleaned.csv", index=False)
from google.colab import files
files.download("df_cleaned.csv")

"""Business Question #1: Does square footage of the living space affect whether a home is expensive?"""

# Business Question: Does square footage affect expensive_home?

# Bin sqft_living into categories (you can adjust bins as needed)
df_encoded['sqft_bin'] = pd.cut(df_encoded['sqft_living'],
                                bins=[0, 1000, 1500, 2000, 2500, 3000, 4000, 6000, np.inf],
                                labels=['<1000', '1000–1500', '1500–2000', '2000–2500', '2500–3000', '3000–4000', '4000–6000', '6000+'])

# Calculate proportion of expensive homes in each bin
bin_summary = df_encoded.groupby('sqft_bin')['expensive_home'].mean().reset_index()

# Plot as bar chart
plt.figure(figsize=(10, 6))
sns.barplot(data=bin_summary, x='sqft_bin', y='expensive_home', palette='pastel')
plt.title('Proportion of Expensive Homes by Square Footage')
plt.xlabel('Square Footage Bins')
plt.ylabel('Proportion of Homes Classified as Expensive')
plt.ylim(0, 1)
plt.tight_layout()
plt.show()


print("Visual Interpretations:")
print("1. Larger homes (especially above 4000 sq ft) are overwhelmingly classified as expensive.")
print("2. There's a sharp increase in expensive classification starting around 2500–3000 sq ft.")
print("3. Homes under 1500 sq ft are rarely considered expensive, typically under 20%.")

"""Business Question #2: Which house age group is most associated with expensive homes?"""

# Boxplot of house price class by house age group
df_cleaned.boxplot(column='expensive_home', by='house_age_group', grid=False)
plt.title('Expensive Home Classification by House Age Group')
plt.suptitle('')
plt.xlabel('House Age Group')
plt.ylabel('Expensive Home (0 = No, 1 = Yes)')
plt.show()

# Visual Interpretations
print("Visual Interpretations:")
print("1. Modern homes make up the vast majority of the dataset and show a clear split between expensive and non-expensive homes.")
print("2. Nearly all 'New' homes are classified as expensive, indicating a strong association between recent construction and high property value.")
print("3. There are no samples labeled as 'Old' or 'Historic' in this dataset, which limits the ability to evaluate trends for older homes.")

"""Business Question #3: Which house grade levels are most associated with expensive homes?"""

# Bar chart of average expensive_home rate by house grade (with rotated x-axis labels)
df_cleaned.groupby('grade')['expensive_home'].mean().plot(
    kind='bar', color='skyblue', edgecolor='black'
)
plt.title('Proportion of Expensive Homes by Grade')
plt.xlabel('House Grade')
plt.ylabel('Proportion of Expensive Homes')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

print("Visual Interpretations:")
print("1. Homes with grades 10 and above are almost always classified as expensive, with proportions close to 1.")
print("2. Mid-tier grades like 8 and 9 show a mix of expensive and non-expensive homes, while grades below 7 are rarely classified as expensive.")
print("3. The steep increase in proportion between grades 8 and 9 suggests that this threshold may be critical for predicting expensive homes.")

"""Pre-Processing"""

# Use only these features
feature_names = [
    'bedrooms', 'bathrooms', 'sqft_living', 'waterfront', 'view',
    'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'lat',
    'sqft_living15', 'sqft_lot15'
]

X = df_cleaned[feature_names]
y = df_cleaned["expensive_home"]


# Confirm shape
print(f"Feature matrix X shape: {X.shape}")
print(f"Target vector y shape: {y.shape}")

"""Encode Target Variable"""

# Only needed if y is not already 0/1
label_encoder = LabelEncoder()

# Encode target
y_encoded = label_encoder.fit_transform(y)

# Check encoding
print(f'Original Classes: {label_encoder.classes_}')
print(f'Encoded Classes: {np.unique(y_encoded)}')

# Replace y with encoded version if needed
y = y_encoded

"""Split the data"""

# Drop string or non-numeric columns from X if any remain
X = X.select_dtypes(include=[np.number])

X = X.drop(columns=['price'], errors='ignore')

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size = 0.3, random_state = 42, stratify = y_encoded)
# Dropping zipcode to not be included as a numeric feature, which may cause problems
X_train = X_train.drop(columns=['zipcode'], errors='ignore')
X_test = X_test.drop(columns=['zipcode'], errors='ignore')

print(f'Training set size: {X_train.shape}')
print(f'Test set size: {X_test.shape}')

"""Standardization"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f'Original feature means: {X_train.mean().round(3).to_dict()}')
print(f'Scaled feature means: {np.mean(X_train_scaled, axis= 0).round(3)}')
print(f'Scaled feature stds: {np.std(X_train_scaled, axis =0).round(3)}')

"""KNN - Model: Finding the optimal K"""

# Test differnt k values using validation curve
k_range = np.arange(1,31, 2) # test odd numbersfrom 1 to 29
k_range

# Use a validation curve to find optimal k
train_scores, val_scores = validation_curve(
                                            KNeighborsClassifier(), X_train_scaled, y_train,
                                            param_name = 'n_neighbors', param_range= k_range,
                                            cv = 5, scoring = 'accuracy', n_jobs = -1
                                          )


# Calculate the mean and std for all validation scores
train_mean = np.mean(train_scores, axis = 1)
train_std = np.std(train_scores, axis = 1)
val_mean = np.mean(val_scores, axis = 1)
val_std = np.mean(np.std(val_scores, axis = 1))

# Find optimal K
optimal_k_idx = np.argmax(val_mean)
optimal_k = k_range[optimal_k_idx]

print(f'\n Optimal K value: {optimal_k}')

print(f'Best Validation accuracy: {val_mean[optimal_k_idx]:.4f}')

# Plot validation Curve for KNN
plt.figure(figsize = (12, 8))
plt.plot(k_range, train_mean, 'o-', color = 'blue', label = 'Training Accuracy')
plt.fill_between(k_range, train_mean - train_std, train_mean + train_std, alpha = 0.1, color= 'blue')
plt.plot(k_range, val_mean, 'o-', color='red', label = 'Validation Accuracy')
plt.fill_between(k_range, val_mean - val_std, val_mean + val_std, alpha = 0.1, color= 'blue')
plt.axvline(x=optimal_k, color='green', linestyle ='--', label=f'Optimal k = {optimal_k}')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.title('KNN Validation Curve: Finding Optimal K ')
plt.legend()
plt.grid(True, alpha = 0.3)
plt.show()

"""Defining Models"""

models = {
          'Logistic Regression': LogisticRegression(random_state = 42, max_iter = 1000),
          'Naive Bayes': GaussianNB(),
          'Decision Tree': DecisionTreeClassifier(random_state = 42),
          'Random Forest': RandomForestClassifier(random_state = 42, n_estimators = 100),
          'SVM (Linear)': SVC(kernel ='linear', random_state =42),
          'SVM (RBF)': SVC(kernel ='rbf', random_state =42),
          'SVM (Sigmoid)': SVC(kernel ='sigmoid', random_state =42),
          'SVM (Poly)': SVC(kernel ='poly', random_state =42),
          'KNN': KNeighborsClassifier(n_neighbors = 25),

}

print(f'Models to be evaluated: {list(models.keys())}')

"""Perform 5 Fold Cross Validation"""

cv_scores = {}
cv_folds = 5
kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)

for name, model in models.items():
  print(f'Evaluating {name}.....')
  scores = cross_val_score(model, X_train_scaled, y_train, cv = kfold, scoring ='accuracy')
  cv_scores[name] = scores
  print(f' CV Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})')
  print()

# Plot Cross-Validation Accuracy for Each Model
cv_means = {model: scores.mean() for model, scores in cv_scores.items()}
cv_stds = {model: scores.std() for model, scores in cv_scores.items()}

plt.figure(figsize=(12, 8))
plt.barh(list(cv_means.keys()), list(cv_means.values()), xerr=list(cv_stds.values()), color='skyblue', edgecolor='black')
plt.xlabel('Mean Accuracy (5-Fold CV)')
plt.title('Cross-Validation Accuracy by Model')
plt.grid(axis='x', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

print("Random Forest achieved the highest cross-validation accuracy (~92%) with the lowest variability,")
print("making it the most accurate and stable model overall; therefore, it is the best choice for deployment.")

"""Train all models and make predictions"""

train_models = {}
predictions = {}
test_accuracies = {}

for name, model in models.items():
  print(f'Training {name}.....')

  # train the model
  model.fit(X_train_scaled, y_train)

  # make predictions
  y_pred = model.predict(X_test_scaled)

  predictions[name] = y_pred

  # calculate test accuracy
  test_accuracy = accuracy_score(y_test, y_pred)
  test_accuracies[name] = test_accuracy

  print(f'Test Accuracy: {test_accuracy:.4f}')
  print()

  print("Conclusion:")
print("Both cross-validation and test set performance confirm that Random Forest is the best-performing")
print("model and should be selected for deployment.")

"""Model Evaluation"""

classification_reports = {}
performance_metrics = {}

target_names = [str(cls) for cls in label_encoder.classes_]  # FIX HERE

for name in models.keys():
    print(f'\n==={name}===')

    # Generate classification report
    report = classification_report(
        y_test,
        predictions[name],
        target_names=target_names,
        output_dict=True
    )

    # Store report and print it
    classification_reports[name] = report
    print(classification_report(y_test, predictions[name], target_names=target_names))

    # Store key performance metrics
    performance_metrics[name] = {
        'accuracy': report['accuracy'],
        'precision': report['weighted avg']['precision'],
        'recall': report['weighted avg']['recall'],
        'f1_score': report['weighted avg']['f1-score']
    }

# Create DataFrame from your stored performance_metrics dictionary
performance_df = pd.DataFrame(performance_metrics).T

# Round for nicer formatting
performance_df = performance_df.round(3)

# Plot grouped bar chart
plt.figure(figsize=(12, 8))
performance_df[['accuracy', 'precision', 'recall', 'f1_score']].plot(kind='bar', figsize=(14, 8))
plt.title('Model vs Weighted Avg of Various Metrics')
plt.ylabel('Score (0 - 1)')
plt.xlabel('Model')
plt.xticks(rotation=45)
plt.legend(title='Metric')
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Export the trained Random Forest model after the loop
import joblib

# Grab the trained Random Forest model from the dictionary
rf = models['Random Forest']

# Save it to a file
joblib.dump(rf, 'random_forest_model.pkl')

"""K-Means Clustering Analysis"""

# Reuse the same k_range used for calculating inertias
k_range = range(1, 15, 2)  # odd values from 1 to 13
inertias = []

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_train_scaled)
    inertias.append(kmeans.inertia_)

# Plot the elbow curve
plt.figure(figsize=(10, 6))
plt.plot(list(k_range), inertias, 'bo-')  # make sure x and y are same length
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Sum of Squared Distances)')
plt.title('K-Means Elbow Method: Finding Optimal k')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

"""Export the trained model"""

import joblib
joblib.dump(rf, 'random_forest_model.pkl')

joblib.dump(scaler, "scaler.pkl")
from google.colab import files
files.download("scaler.pkl")
